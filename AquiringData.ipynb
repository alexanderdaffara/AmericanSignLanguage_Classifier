{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>participant_id</th>\n",
       "      <th>sequence_id</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_landmark_files/26734/1000035562.parquet</td>\n",
       "      <td>26734</td>\n",
       "      <td>1000035562</td>\n",
       "      <td>blow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_landmark_files/28656/1000106739.parquet</td>\n",
       "      <td>28656</td>\n",
       "      <td>1000106739</td>\n",
       "      <td>wait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_landmark_files/16069/100015657.parquet</td>\n",
       "      <td>16069</td>\n",
       "      <td>100015657</td>\n",
       "      <td>cloud</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_landmark_files/25571/1000210073.parquet</td>\n",
       "      <td>25571</td>\n",
       "      <td>1000210073</td>\n",
       "      <td>bird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_landmark_files/62590/1000240708.parquet</td>\n",
       "      <td>62590</td>\n",
       "      <td>1000240708</td>\n",
       "      <td>owie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            path  participant_id  sequence_id  \\\n",
       "0  train_landmark_files/26734/1000035562.parquet           26734   1000035562   \n",
       "1  train_landmark_files/28656/1000106739.parquet           28656   1000106739   \n",
       "2   train_landmark_files/16069/100015657.parquet           16069    100015657   \n",
       "3  train_landmark_files/25571/1000210073.parquet           25571   1000210073   \n",
       "4  train_landmark_files/62590/1000240708.parquet           62590   1000240708   \n",
       "\n",
       "    sign  \n",
       "0   blow  \n",
       "1   wait  \n",
       "2  cloud  \n",
       "3   bird  \n",
       "4   owie  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(94477, 4)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = pd.read_csv('data/train.csv')\n",
    "display(y.head())\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dataset(dataset, signs, col_names):\n",
    "    print(f'recordings found: {y.loc[[s in signs for s in y.sign]].shape[0]}')\n",
    "    for i, sign in enumerate(signs):\n",
    "        print(f'sign: {sign}, {i+1}/{len(signs)}')\n",
    "        for j, participant_id in enumerate(y.participant_id.unique()):\n",
    "            print(f'    pid: {j+1}/21')\n",
    "            y_subset = y.loc[(y.sign == sign) & (y.participant_id == participant_id)]\n",
    "            data = []\n",
    "            n = y_subset.shape[0]\n",
    "            for k, path in enumerate(y_subset.path):\n",
    "                print(f'        recording: {k+1}/{n}')\n",
    "                path_D = \"D:/\" + path\n",
    "                df = pd.read_parquet(path_D)\n",
    "                this_data = []\n",
    "                for type in set(df.type):\n",
    "                    df_t = df.loc[df.type == type]\n",
    "                    for lm_i in set(df_t.landmark_index):\n",
    "                        df_t_i = df_t.loc[(df_t['x'].notna()) & (df_t['y'].notna()) & (df_t.landmark_index == lm_i)]\n",
    "                        if df_t_i.shape[0] != 0:\n",
    "                            this_data.append(df_t_i.iloc[0]['x'])\n",
    "                            this_data.append(df_t_i.iloc[0]['y'])\n",
    "                            this_data.append(df_t_i.iloc[-1]['x'] - df_t_i.iloc[0]['x'])\n",
    "                            this_data.append(df_t_i.iloc[-1]['y'] - df_t_i.iloc[0]['y'])\n",
    "                        else:\n",
    "                            this_data.extend([0,0,0,0])\n",
    "                data.append(this_data + [sign])\n",
    "            sign_df = pd.DataFrame(data, columns=col_names)\n",
    "            dataset = pd.concat((dataset,sign_df), axis=0, ignore_index=True)\n",
    "    return dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Signs used on round 1 of data collection\n",
    "['hungry',\n",
    " 'stay',\n",
    " 'drink',\n",
    " 'dad',\n",
    " 'sad']\n",
    "\n",
    "#### Signs used on round 2 of data collection\n",
    "['pig',\n",
    " 'elephant',\n",
    " 'mad',\n",
    " 'finger',\n",
    " 'green',\n",
    " 'up',\n",
    " 'bad',\n",
    " 'finger',\n",
    " 'bye',\n",
    " 'airplane',\n",
    " 'mom',\n",
    " 'kitty',\n",
    " 'home',\n",
    " 'yellow',\n",
    " 'shirt',\n",
    " 'talk',\n",
    " 'icecream',\n",
    " 'green',\n",
    " 'clown',\n",
    " 'lion',\n",
    " 'bad',\n",
    " 'thirsty',\n",
    " 'elephant',\n",
    " 'book',\n",
    " 'shirt',\n",
    " 'dog',\n",
    " 'finger',\n",
    " 'pretty',\n",
    " 'vacuum',\n",
    " 'vacuum',\n",
    " 'chocolate',\n",
    " 'pretty',\n",
    " 'same',\n",
    " 'red',\n",
    " 'every',\n",
    " 'feet',\n",
    " 'drawer',\n",
    " 'gum',\n",
    " 'drop',\n",
    " 'story',\n",
    " 'puppy',\n",
    " 'haveto',\n",
    " 'book',\n",
    " 'bad',\n",
    " 'zebra',\n",
    " 'cat',\n",
    " 'cow',\n",
    " 'mad',\n",
    " 'dryer',\n",
    " 'old',\n",
    " 'clown',\n",
    " 'garbage',\n",
    " 'stairs',\n",
    " 'cute',\n",
    " 'wolf',\n",
    " 'for',\n",
    " 'tree',\n",
    " 'stairs',\n",
    " 'sick',\n",
    " 'finish',\n",
    " 'bird',\n",
    " 'nose',\n",
    " 'no',\n",
    " 'drop',\n",
    " 'yellow',\n",
    " 'garbage',\n",
    " 'have',\n",
    " 'blow',\n",
    " 'wet',\n",
    " 'airplane',\n",
    " 'vacuum',\n",
    " 'no',\n",
    " 'bye',\n",
    " 'nose',\n",
    " 'give',\n",
    " 'puppy',\n",
    " 'gum',\n",
    " 'dry',\n",
    " 'shower',\n",
    " 'another',\n",
    " 'every',\n",
    " 'haveto',\n",
    " 'wolf',\n",
    " 'bee',\n",
    " 'empty',\n",
    " 'puppy',\n",
    " 'thirsty',\n",
    " 'night',\n",
    " 'gum',\n",
    " 'sick',\n",
    " 'drawer',\n",
    " 'nose',\n",
    " 'have',\n",
    " 'pretty',\n",
    " 'airplane',\n",
    " 'cry',\n",
    " 'tiger',\n",
    " 'mom',\n",
    " 'talk',\n",
    " 'have',\n",
    " 'green',\n",
    " 'zebra',\n",
    " 'because',\n",
    " 'gum',\n",
    " 'puppy',\n",
    " 'mad',\n",
    " 'wolf',\n",
    " 'donkey',\n",
    " 'yes',\n",
    " 'yucky',\n",
    " 'why',\n",
    " 'no',\n",
    " 'tree',\n",
    " 'tree',\n",
    " 'dryer',\n",
    " 'morning',\n",
    " 'that',\n",
    " 'orange',\n",
    " 'where',\n",
    " 'wolf',\n",
    " 'tomorrow',\n",
    " 'tomorrow',\n",
    " 'gum',\n",
    " 'uncle',\n",
    " 'shower',\n",
    " 'mouth',\n",
    " 'nose',\n",
    " 'farm',\n",
    " 'frog',\n",
    " 'table',\n",
    " 'green',\n",
    " 'another',\n",
    " 'helicopter',\n",
    " 'talk',\n",
    " 'dry',\n",
    " 'hear',\n",
    " 'yourself',\n",
    " 'cute',\n",
    " 'gum',\n",
    " 'garbage',\n",
    " 'dryer',\n",
    " 'find',\n",
    " 'kitty',\n",
    " 'chocolate',\n",
    " 'no',\n",
    " 'bird',\n",
    " 'girl',\n",
    " 'story',\n",
    " 'red',\n",
    " 'cry',\n",
    " 'story',\n",
    " 'yourself',\n",
    " 'vacuum',\n",
    " 'green',\n",
    " 'another']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recordings found: 1\n",
      "sign: hungry, 1/1\n",
      "    pid: 1/21\n",
      "        recording: 1/1\n"
     ]
    }
   ],
   "source": [
    "col_names = pd.read_csv('data/col_names.csv', index_col=0)['0']\n",
    "dataset = pd.DataFrame([],columns=col_names)\n",
    "signs = pd.read_csv('data/all_targets.csv',index_col=0)['0'].to_list()\n",
    "\n",
    "dataset = update_dataset(dataset,signs,col_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load dataset to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.to_csv('data/dataset_test_hungry.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
